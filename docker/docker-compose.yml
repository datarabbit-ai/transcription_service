services:
  redis:
    image: redis:6.2-alpine
    ports:
      - "6380:6379"   # Expose Redis on a different port to avoid conflicts with local Redis instances
    volumes:
      - ./volumes/redis_data:/data
    command: redis-server --appendonly yes
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 3

  api:
    build:
      context: ..
      dockerfile: docker/Dockerfile.base
    command: ["uvicorn", "transcription_service.main:app", "--host", "0.0.0.0", "--port", "8000"]
    ports:
      - "8001:8000"  # Expose the API on a different port to avoid conflicts with local services
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=10
      - UPLOADS_DIR=/app/uploads
      - TRANSCRIPTIONS_DIR=/app/transcriptions
    volumes:
      - ./volumes/api_uploads:/app/uploads
      - ./volumes/api_transcriptions:/app/transcriptions
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://api:8000/ping"]
      interval: 10s
      timeout: 10s
      retries: 3

  worker:
    build:
      context: ..
      dockerfile: docker/Dockerfile.base
    command: ["python", "transcription_service/worker.py"]
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=10
      - UPLOADS_DIR=/app/uploads
      - TRANSCRIPTIONS_DIR=/app/transcriptions
      - WHISPER_MODEL_NAME=large-v3
      - WHISPER_MODEL_DEVICE=cuda
    volumes:
      - ./volumes/api_uploads:/app/uploads
      - ./volumes/api_transcriptions:/app/transcriptions
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
    runtime: nvidia
    depends_on:
      redis:
        condition: service_healthy
